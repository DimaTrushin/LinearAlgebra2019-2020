\ProvidesFile{lecture23.tex}[Лекция 23]

\paragraph{Матричный формализм}

Пусть $\beta\colon V\times U\to F$ -- билинейная форма и пусть $v = (v_1,\ldots,v_s)$ -- некоторый набор векторов из $V$ и $u = (u_1,\ldots,u_t)$ -- набор векторов из $U$. Тогда рассмотрим следующую конструкцию
\[
v^t\cdot_\beta u = 
\begin{pmatrix}
{v_1}\\{\vdots}\\{v_s}
\end{pmatrix}
\cdot_\beta
\begin{pmatrix}
{u_1}&{\ldots}&{u_t}
\end{pmatrix}
=
\begin{pmatrix}
{v_1\cdot_\beta u_1}&{\ldots}&{v_1\cdot_\beta u_t}\\
{\vdots}&{\ddots}&{\vdots}\\
{v_s\cdot_\beta u_1}&{\ldots}&{v_s\cdot_\beta u_t}\\
\end{pmatrix}
\]
То есть мы умножаем столбец из векторов из $V$ на строку из векторов из $U$ с помощью билинейной формы, рассматриваемой как оператор умножения. Тогда результат будет матрица из $\operatorname{M}_{s\,t}(F)$. Причем умножение происходит по тем же самым формальным правилам, что и обычное матричное умножение, только с использованием $\cdot_\beta$ вместо обычного умножения (которое для векторов даже не определено).



Тогда, если выбрать $e = (e_1,\ldots,e_n)$ -- базис $V$ и $f = (f_1,\ldots,f_m)$ -- базис $U$, то 
\[
B_\beta = e^t\cdot_\beta f = 
\begin{pmatrix}
{e_1}\\{\vdots}\\{e_n}
\end{pmatrix}
\cdot_\beta
\begin{pmatrix}
{f_1}&{\ldots}&{f_m}
\end{pmatrix}
\]
Мы привыкли, что в случае линейных отображений вычисления можно вести в удобной матричной форме. Последнее равенство позволяет вычисления с билинейными формами сводить к матричной.

\begin{claim}
Пусть $V$ и $U$ -- векторные пространства над полем $F$, $e_1,\ldots,e_n \in V$ -- базис $V$ и $f_1,\ldots,f_m\in U$ -- базис $U$. Тогда для любого набора чисел $b_{ij}\in F$, где $1\leqslant i \leqslant n$ и $1\leqslant j \leqslant m$, существует единственная билинейная форма $\beta\colon V\times U\to F$ такая, что $\beta(e_i,f_j) = b_{ij}$.
\end{claim}
\begin{proof}
По сути -- это переформулировка утверждения~\ref{claim::BilinearMatrices}.
\end{proof}


\begin{claim}
Пусть $\beta\colon V\times U\to F$ -- билинейная форма. Пусть в пространстве $V$ зафиксировано два базиса $e=(e_1,\ldots,e_n)$ и $e' = (e_1',\ldots,e_n')$ с матрицей перехода $C\in \operatorname{M}_n(F)$ такой, что $e'=eC$, пусть в пространстве $U$ также зафиксированы два базиса $f = (f_1,\ldots,f_m)$ и $f'=(f_1',\ldots,f_m')$ с матрицей перехода $D\in \operatorname{M}_m(F)$ такой, что $f' = fD$. Если $B_\beta$ -- матрица $\beta$ в базисах $e$ и $f$ и $B_\beta'$ -- матрица $\beta$ в базисах $e'$ и $f'$, тогда $B_\beta' = C^t B_\beta D$.
\end{claim}
\begin{proof}
Пользуясь только что введенным формализмом можно проделать следующие вычисления:\footnote{Обратите внимание, что тут у нас присутствует два умножения: матричное с числами и матричное с билинейной формой. Порядок этих операций (то есть расстановка скобок) не важны, это следует просто из определения билинейной формы, если присмотреться внимательно.}
\[
B_\beta' = (e')^t\cdot_\beta f' = (eC)^t \cdot_\beta (f D) = (C^t e^t) \cdot_\beta (f D) = C^t(e^t \cdot_\beta f) D = C^t B_\beta D
\]
\end{proof}

\paragraph{Замечания}
\begin{itemize}
\item Заметим, что если билинейная форма определена на одном пространстве $\beta\colon V\times V\to F$, то достаточно выбрать один базис $e=(e_1,\ldots,e_n)$, после чего коэффициенты $B_\beta$ считаются по правилу $b_{ij} = \beta(e_i,e_j)$. При этом, если $e'=(e_1',\ldots,e_n')$ -- другой базис и $e'=eC$, где $C$ -- матрица перехода, то $B_\beta' = C^t B C$.

\item Пусть у нас есть два векторных пространства $V$ и $U$. Тогда на них могут жить два разного рода объектов: линейные отображения и билинейные формы, например, $\phi\colon V\to U$ и $\beta\colon U\times V \to F$. Если мы выберем базис в $V$ и базис в $U$, то $V$ превращается в $F^n$, а $U$ -- в $F^m$. В этом случае, линейное отображение $\phi$ описывается некоторой матрицей $A\in\operatorname{M}_{m\,n}(F)$, при этом $\phi(x) = Ax$. С другой стороны, билинейная форма тоже описывается матрицей $B\in \operatorname{M}_{m\,n}(F)$, при этом $\beta(x,y) = x^tBy$.

Таким образом, для описания и линейных отображений и билинейных форм в фиксированном базисе мы используем матрицы (причем одного и того же размера). Возникает вопрос: <<а как понять,  когда матрица задает линейное отображение, а когда билинейную форму?>> Если  нам выдали только одну пару базисов и матрицу $S$, то ответ простой -- никак. В фиксированном базисе они не отличимы. Мы можем считать нашу матрицу $S$ линейным оператором или билинейной формой, в зависимости от наших предпочтений. Однако, если нам выдали несколько базисов, например два, и в этих базисах наш объект задается матрицами $S$ и $S'$. То отличить оператор от билинейной формы можно по формуле перехода, а именно, если задан оператор, то $S' = D^{-1}S C$, а если билинейная форма, то $S' = C^t S D$. Конечно, если базисы трепетно подобраны (врагом или другом -- это как повезет), то мы все равно можем не заметить разницы. Но если мы будем сравнивать во всех возможных базисах, то ответ определяется однозначно.
\end{itemize}

\subsection{Симметричность и кососимметричность}

\begin{definition}
Пусть $\beta\colon V\times V\to F$ -- билинейная форма определенная на одном пространстве. Тогда
\begin{itemize}
\item Будем говорить, что $\beta$ симметричная, если $\beta(u,v) = \beta(v,u)$ для любых $u,v\in V$.

\item Будем говорить, что $\beta$ кососимметрична, если $\beta(v,v) = 0$ для любого $v\in V$.
\end{itemize}
\end{definition}

\paragraph{Замечание}
Давайте обсудим свойство кососимметричности. Вас должно было удивить такое дурацкое условие в определении. Однако, теперь, когда мы уже взрослые и знаем разные поля, но толком про них ничего еще и не знаем, нам надо быть чуточку аккуратными. Если $\beta(v,v) = 0$ для любого $v\in V$, тогда $\beta(v+u,v+u) = 0$ для любых $u,v\in V$. Это значит
\[
0=\beta(v+u,v+u) = \beta(v,v) + \beta(v,u) + \beta(u,v)+\beta(u,u) = \beta(v,u) + \beta(u,v)
\]
То есть из этого условия вытекает, что $\beta(v,u) = -\beta(u,v)$. Однако, если нам дано, что $\beta(v,u) = -\beta(u,v)$, то подставив $u = v$, мы получим $\beta(v,v) = -\beta(v,v)$, а значит $2\beta(v,v) = 0$. И вот тут начинаются чудеса. Бывают поля, в которых $2 = 0$. В таких полях условие $\beta(v,v) = 0$ сильнее условия $\beta(v,u) = -\beta(u,v)$. Правильное определение -- потребовать более сильное условие, которое и содержится в определении.

\paragraph{Матрицы симметричных и кососимметричных форм}

\begin{claim}\label{claim::BilSymAntiSym}
Пусть $\beta\colon V\times V\to F$ -- билинейная форма. Тогда
\begin{enumerate}
\item Билинейная форма $\beta$ симметрична тогда и только тогда, когда матрица $B_\beta$ симметрична $B_\beta^t = B_\beta$.

\item Если $2\neq 0$ в поле $F$, то билинейная форма кососимметрична тогда и только тогда, когда матрица $B_\beta$ кососимметрична $B_\beta^t = - B_\beta$.

\item Если $2 = 0$ в поле $F$, то билинейная форма кососимметрична тогда и только тогда, когда матрица $B_\beta$ симметрична $B_\beta^t = B_\beta$ и имеет нулевую диагональ.\footnote{\label{foot::AntiSymMatrix}Над полем $F$, где $2 = 0$, эти два условия по определению полагаются условием кососимметричности матрицы. Тогда получается, что билинейная форма кососимметрична тогда и только тогда, когда матрица кососимметрична.}
\end{enumerate}
\end{claim}
\begin{proof}
(1) Фиксируем базис $e_1,\ldots,e_n$, тогда билинейная форма превращается в $\beta(x,y) = x^t B y$. Условие $\beta(x,y) = \beta(y,x)$ для любых $x,y\in F^n$ равносильно условию $x^t B y = y^t B x = (y^t B x)^t = x^t B^t y$ для любых $x,y\in F^n$. Заметим, что это условие равносильно симметричности матрицы $B$, которая в свою очередь является матрицей $B_\beta$ в базисе $e_1,\ldots,e_n$.

(2) Если $2\neq 0$ в поле $F$, то кососимметричность равносильна условию $\beta(x,y) = -\beta(y,x)$. Выберем произвольный базис $e_1,\ldots,e_n$, тогда $\beta(x,y) = x^t By$. Тогда условие $\beta(x,y) = -\beta(y,x)$ для всех $x,y\in F^n$ равносильно условию $x^t By = -y^t Bx = -(y^t B x)^t =- x^t B^t y$ для любых $x,y\in F^n$. Последнее равносильно условию $B^t = - B$.

(3) Пусть теперь $2 = 0$ в $F$. Тогда надо заметить, что $1 + 1 = 0$, то есть $1 = - 1$. То есть условие $B^t = B$ и $B^t = - B$ равносильны! Как и выше выберем некоторый базис $e_1,\ldots,e_n$ и в нем запишем нашу форму в виде $\beta(x,y) = x^t B y$. Если $\beta$ кососимметрична, то $\beta(x,y) = -\beta(y,x) = \beta(y,x)$, то есть форма симметрична. А значит $B^t = B$ по первому пункту. С другой стороны, так как $\beta(e_i,e_i) = 0$, то диагональ матрицы $B$ должна быть нулевой.

Обратно. Этот случай противной математики, когда без счета в лоб не обойтись (сочувствую нам всем). Пусть $v = x_1e_1+\ldots+x_ne_n$ -- некоторый вектор, посчитаем $\beta(v,v)$ и покажем, что мы получим ноль.
\begin{gather*}
\beta(v,v) = \beta(\sum_i x_i e_i, \sum_j x_j e_j) = \sum_{ij}x_ix_j \beta(e_i,e_j) =\\ \sum_{i=1}^nx_i^2 \beta(e_i, e_i) + \sum_{i<j}(x_i x_j \beta(e_i, e_j) + x_j x_i \beta(e_j, e_i)) = \sum_{i=1}^nx_i^2 \beta(e_i, e_i) + 2\sum_{i<j}x_i x_j \beta(e_i, e_j)
\end{gather*}
Тогда в последней сумме первое слагаемое равно нулю так как диагональ $B_\beta$ состоит из нулей, а второе равно нулю, так как $2 = 0$.
\end{proof}

Как мы видим в случае $2 = 0$ в поле $F$ нас ждал сюрприз. Самым большим откровением обычно становится тот факт, что кососимметрические билинейные формы в данном случае становятся специального вида симметрическими формами. Вот такие чудесатые чудеса.

\subsection{Матричные характеристики билинейной формы}\label{subsection::BilChar}

В случае линейного отображения или оператора мы поступали так: выбирали базисы (или базис) и задавали их матрицами. Потом считали какие-то характеристики этих самых матриц и показывали, что они не зависят от базисов. Давайте попробуем такой же подход в случае с билинейными формами.

\paragraph{Ранг}

Пусть $\beta\colon V\times U\to F$ -- билинейная форма и в каких то парах базисов она задана матрицами $B$ и $B'$. Тогда $B' = C^t B D$ для некоторых невырожденных матриц $C$ и $D$. Тогда $\rk B' = \rk B$, так как он не меняется при умножении слева и справа на невырожденную матрицу (утверждение~\ref{claim::rkInvariance}).

Теперь давайте считать, что $\beta\colon V\times V\to F$ определена на одном пространстве и в разных базисах задается квадратными матрицами $B$ и $B'$.

\paragraph{След}
Так как $\tr(B') = \tr(C^t B C)$, то вообще говоря след не несет никакой содержательной информации. Действительно, рассмотрите пример $B = \begin{pmatrix}{1}&{0}\\{0}&{-1}\end{pmatrix}$ и $C = \begin{pmatrix}{a}&{b}\\{c}&{d}\end{pmatrix}$ -- невырожденная матрица, то есть $ad - bc\neq 0$. Тогда $\tr(B) = 0$ и $\tr(B') = (a^2 + b^2) - (c^2 + d^2)$. В случае поля $\mathbb R$ или $\mathbb C$ это означает, что след $B'$ может быть каким угодно числом.

\paragraph{Определитель}

В этом случае $\det(B') = \det(C^t B C) = \det(B) \det (C)^2$. То есть определитель в разных базисах может отличаться на квадрат числа из $F^*$. В общем случае это означает, что корректно определено понятие $\det(B) = 0$ или $\det(B)\neq 0$. Например:
\begin{itemize}
\item Если поле $F = \mathbb C$, то условие $\det(B) = 0$ или $\det(B)\neq 0$ -- лучшее, что можно получить. Действительно, если $\det(B)\neq 0$, то в другом базисе $\det(B') = \det(B)c^2$. Так как в поле $\mathbb C$ из любого числа можно извлечь корень, то $\det(B')$ можно сделать произвольным.
\item Пусть $F = \mathbb R$, тогда корректно определен знак определителя, то есть $\sgn \det B$ корректно определен. Действительно, $\det(B') = \det(B) c^2$. В поле $\mathbb R$ число равно квадрату тогда и только тогда, когда оно положительно.
\item Пусть $F = \mathbb Q$. Тут ничего особенно сказать нельзя, кроме явной формулировки. Пусть $\det(B) = p_1^{k_1}\ldots p_r^{k_r}$, где $p_i\in \mathbb N$ -- простые числа, а $k_i\in \mathbb Z$ -- степени (вообще говоря положительные или отрицательные). Тогда четности чисел $k_i$ определены однозначно независимо от базиса.
\end{itemize}

Как мы видим определитель вообще говоря не определен для билинейной формы, но какие-то характеристики из него вытащить можно и эти характеристики сильно зависят от поля, над которым мы работаем.

\paragraph{Обратимость матрицы}
Так как корректно определен ранг или понятие $\det(B) = 0$, то обратимость матрицы $B$ тоже не зависит от базиса.

\paragraph{Симметричность и кососимметричность}

Симметричность и кососимметричность матрицы билинейной формы не зависит от базиса в силу утверждения~\ref{claim::BilSymAntiSym} (смотрите также сноску~\ref{foot::AntiSymMatrix} в этом утверждении).

\subsection{Ортогональные дополнения и ядра}

\begin{definition}
Пусть $\beta\colon V\times U\to F$ -- билинейная форма. Тогда
\begin{itemize}
\item Если $X\subseteq V$ -- произвольное подмножество, тогда правое ортогональное дополнение к $X$ это
\[
X^\bot = \{u\in U \mid \beta(x,u) = 0\,\forall x\in X\} = \{u\in U\mid \beta(X,u) = 0\}
\]
\item Если $Y\subseteq U$ -- произвольное подмножество, тогда левое ортогональное дополнение к $Y$ это
\[
{}^\bot Y = \{v\in V\mid \beta(v, y) = 0\,\forall y\in Y\} = \{v\in V\mid \beta(v, Y) = 0\}
\]
\end{itemize}
\end{definition}

\paragraph{Замечания}
\begin{itemize}
\item
Когда понятно о чем идет речь и нет путаницы, обычно оба дополнения обозначают $X^\bot$ и $X^\bot$. Обычно это не мешает если пространства $V$ и $U$ разные, так как каждое дополнение живет в своем отдельном пространстве. Однако, если форма определена на одном пространстве $\beta\colon V\times V\to F$, то приходится использовать разные обозначения, так как для $X\subseteq V$ определены оба дополнения $X^\bot$ и ${}^\bot X$ и оба живут в $V$. Я постараюсь различать дополнения там, где это необходимо.

\item 
Стоит отметить, что ортогональное дополнение $X^\bot$ к подмножеству $X\subseteq V$ обязательно будет подпространством в $U$, аналогично и для второго дополнения.
\end{itemize}


\paragraph{Пример}
Давайте выясним как считать левое и правое ортогональные дополнения к подпространству. Пусть $\beta\colon F^n\times F^m \to F$ -- некоторая билинейная форма заданная $\beta(x,y) = x^t B y$, где $B\in \operatorname{M}_{n\,m}(F)$. Пусть $W = \langle w_1,\ldots,w_s\rangle\subseteq F^n$ -- некоторое подпространство заданное в виде линейной оболочки. Положим $T = (w_1|\ldots|w_s)\in\operatorname{M}_{n\,s}(F)$ -- матрица из столбцов $w_i$. Тогда 
\[
W^\bot = \{y\in F^m \mid T^t By = 0\}
\]
Аналогично, если $U=\langle u_1,\ldots,u_r \rangle \subseteq F^m$ -- подпространство и $P = (u_1|\ldots|u_r)$ -- матрица из столбцов $u_i$. То
\[
{}^\bot U = \{x\in F^n\mid x^t B P = 0\} = 
\{x\in F^n \mid P^t B^t x = 0\}
\]


\begin{definition}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма, тогда ее правым ядром называется $\ker^R \beta =V^\bot$, а левым ядром $\ker^L\beta = {}^\bot U$.
\end{definition}

Смысл левого ядра в том, что это такие векторы из $V$, которые на что ни умножай, все равно получишь $0$. В этом смысле -- это не интересные векторы, изучение которых с точки зрения билинейной форм невозможно. Может быть эти векторы при умножении с другой стороны дадут что-то, но это уже отдельный вопрос.

\paragraph{Пример}
Пусть $\beta\colon F^n\times F^m \to F$ -- некоторая билинейная форма заданная правилом $\beta(x,y) = x^t By$, где $B\in \operatorname{M}_{n\,m}(F)$. Тогда
$\ker^R \beta = \{y\in F^m\mid By = 0\}$ и $\ker^L\beta = \{x\in F^n \mid x^t B = 0\} = \{x\in F^n \mid B^t x = 0\}$.

\begin{definition}
Билинейная форма $\beta\colon V\times U\to F$ называется невырожденной, если $\ker^R\beta = 0$ и $\ker^L\beta = 0$.
\end{definition}


\begin{claim}
Билинейная форма $\beta\colon V\times U\to F$ невырождена тогда и только тогда, когда $\dim V = \dim U$ и матрица формы $\beta$ невырождена.
\end{claim}
\begin{proof}
Пусть форма $\beta$ невырожденная. Выберем базисы в $V$ и $U$, тогда наша билинейная форма превратится в $\beta\colon F^n \times F^m\to F$ по правилу $(x,y)\mapsto x^t B y$ для некоторой матрицы $B\in \operatorname{M}_{n\,m}(F)$. Тогда
\[
\ker^R \beta = \{y\in F^m\mid By = 0\}\quad\text{и}\quad \ker^L\beta = \{x\in F^n \mid B^t x = 0\}
\]
Если размерности пространств разные, то матрица $B$ не квадратная и хотя бы одно из ядер не ноль, так как хотя бы одна из систем $B y = 0$ или $B^t x = 0$ содержит переменных больше чем уравнений, а значит есть ненулевое решение. Теперь мы знаем, что $B$ квадратная и система $By = 0$ имеет только нулевые решения, значит $B$ -- невырожденная матрица по утверждению~\ref{claim::InvertibleDiscription}.

Обратно, пусть $\dim V = \dim U$ и матрица $B_\beta$ не вырождена. Тогда в каких-то координатах $\beta$ записывается так $\beta\colon F^n \times F^n \to F$ по правилу $(x,y)\mapsto x^t By$. Так как $B$ невырожденная, то системы $B y = 0$ и $B^t x = 0$ имеют только нулевые решения, значит оба ядра нулевые, значит форма невырожденная.

\end{proof}

\begin{claim}\label{claim::BilinearKernels}
Пусть $\beta\colon V\times U\to F$ -- некоторая билинейная форма, тогда:\footnote{Это утверждение является прямым аналогом утверждения~\ref{claim::ImKer} для линейных отображений и является очередным проявлением тривиального наблюдения для систем линейных уравнений, что количество главных и свободных переменных равно количеству всех переменных.}
\begin{enumerate}
\item $\dim\ker^L \beta + \rk \beta = \dim V$
\item $\dim\ker^R \beta + \rk \beta = \dim U$
\end{enumerate}
\end{claim}
\begin{proof}
Докажем для определенности первое утверждение, другое ему симметрично. Записав все в координатах, мы имеем $\beta \colon F^n \times F^m \to F$ по правилу $\beta(x,y) = x^t By$. И для левого ядра мы имеем $\ker^L \beta = \{x\in F^n \mid B^t x = 0\}$. Тогда $\dim \ker^L\beta$ -- количество свободных переменных системы $B^t x=0$ (раздел~\ref{section::Subspaces} о ФСР), $\rk \beta$ -- количество главных переменных системы $B^t x = 0$ (совпадает со строчным рангом $B^t$), а $\dim V $ -- количество переменных системы $B^t x = 0$. Ну а количество главных плюс количество свободных переменных -- это все переменные.
\end{proof}

\subsection{Двойственность для подпространств}

\begin{claim}\label{claim::DualitySpaces}
Пусть $\beta\colon V\times U\to F$ -- невырожденная билинейная форма. Тогда:
\begin{enumerate}
\item Для любого подпространства $W\subseteq V$ выполнено
\[
\dim W^\bot + \dim W = \dim V
\]
\item Для любого подпространства $W\subseteq V$ выполнено ${}^\bot(W^\bot) = W$.
\item Для любых подпространств $W\subseteq E\subseteq V$ верно, что $W^\bot \supseteq E^\bot$. Причем $W = E$ тогда и только тогда, когда $W^\bot = E^\bot$.
\item Для любых подпространств $W, E\subseteq V$ выполнено равенство
\[
(W + E)^\bot = W^\bot \cap E^\bot
\]
\item Для любых подпространств $W, E\subseteq V$ выполнено равенство
\[
(W\cap E)^\bot = W^\bot + E^\bot
\]
\end{enumerate}
Аналогично выполнены все свойства для подпространств $W\subseteq U$ и их левых ортогональных дополнений ${}^\bot W$.
\end{claim}
\begin{proof}
Давайте прежде всего перейдем в координаты выбрав какой-нибудь базис $V$ и $U$. Тогда получим $\beta\colon F^n \times F^n \to F$ по правилу $\beta(x,y) = x^t B y$, где $B\in \operatorname{M}_n(F)$ -- невырожденная матрица.

(1) Пусть $W = \langle w_1,\ldots,w_r \rangle$ задано своим базисом и $T = (w_1|\ldots|w_r)\in \operatorname{M}_{n\,r}(F)$. Тогда $W^\bot = \{y\in F^n \mid T^tB y = 0\}$. Так как матрица $B$ невырожденная, то $\rk(T^t B) = \rk (T^t) = r$ (утверждение~\ref{claim::rkInvariance}). В очередной раз все интерпретируем в терминах свободных и главных переменных системы $T^t By = 0$. Имеем: $\dim W = r$ -- это количество главных переменных системы, $\dim W^\bot$ -- это количество свободных переменных, а $\dim V$ -- это количество всех переменных, что и требовалось.

(2) Давайте в начале покажем, что $W \subseteq {}^\bot(W^\bot)$, а потом сравним их размерности. Пусть $w\in W$, нам надо показать, что $w\in {}^\bot (W^\bot)$. То есть нам надо показать, что $\beta(w, W^\bot) = 0$. То есть для любого $v\in W^\bot$ надо показать, что $\beta(w,v) = 0$. Однако, по определению, если $v\in W^\bot$, то $\beta(w, v) = 0$ для любого $w\in W$, что и требовалось. Теперь надо показать, что пространства имеют одинаковую размерность. Для этого воспользуемся пунктом~(1):
\[
\dim {}^\bot (W^\bot) = \dim U - \dim W^\bot = \dim U- (\dim V - \dim W) = \dim W
\]
последнее равенство в силу того, что $\dim U = \dim V$. А раз пространства вложены и имеют одинаковую размерность, то они совпадают.

(3) Пусть $W\subseteq E\subseteq V$, тогда
\[
W^\bot = \{u\in U\mid \beta(w, u) = 0,\,w\in W\} \quad \text{и} \quad E^\bot = \{u\in U\mid \beta(e, u) = 0,\,e\in E\}
\]
Заметим, что так как $W\subseteq E$, то справа ограничений не меньше, чем слева, а значит пространство не больше.

Пусть теперь $E, W\subseteq V$ -- произвольные подпространства. Тогда если они равны, то и их ортогональные дополнения равны. Обратно, пусть $W^\bot = E^\bot$, тогда ${}^\bot(W^\bot) = {}^\bot(E^\bot)$, то есть по пункту~(2) $W = E$.

(4) Рассмотрим левую и правую части равенства $(W + E)^\bot = W^\bot\cap E^\bot$ отдельно:
\[
(W+E)^\bot = \{u\in U\mid \beta(w + e, u) = 0,\,\forall w\in W,\,\forall e\in E\}
\]
С другой стороны
\begin{gather*}
W^\bot\cap E^\bot = \{u\in U\mid \beta(w, u) = 0,\,\forall w\in W\}\cap \{u\in U\mid \beta(e,u) = 0,\,\forall e\in E\} =\\
\{u\in U\mid \beta(w,u) = 0,\forall w\in W\text{ и }\beta(e,u)=0,\,\forall e\in E\}
\end{gather*}
Если $u\in W^\bot\cap E^\bot$, то $\beta(w,u) = 0$ и $\beta(e,u) = 0$ для любых $w\in W$ и $e\in E$, а значит и $\beta(w + e, u) = 0$, то есть $u\in (W+E)^\bot$. Обратно, если $u\in (W+E)^\bot$, то $\beta(w+e,u) = 0$ для любых $w\in W$ и $e\in E$. В частности для любого $w\in W$ и $e = 0\in E$ получаем $\beta(w, u) = 0$, аналогично $w = 0\in W$ и любого $e \in E$ получаем $\beta(e, u) = 0$. То есть $u\in W^\bot\cap E^\bot$.

(5) Выведем это утверждение из предыдущего с помощью остальных. Действительно, чтобы доказать равенство $(W\cap E)^\bot = W^\bot + E^\bot$, необходимо и достаточно доказать ${}^\bot((W\cap E)^\bot) = {}^\bot(W^\bot + E^\bot)$ по пункту~(3) вторая часть. В силу~(2) это равносильно $W\cap E = {}^\bot(W^\bot+E^\bot)$. Опять воспользовавшись~(2), получаем, что это равносильно ${}^\bot(W^\bot) \cap {}^\bot(E^\bot) = {}^\bot(W^\bot + E^\bot)$. А это симметричное утверждение утверждению~(4) для подпространств $W^\bot,E^\bot\subseteq U$.

\end{proof}
