\ProvidesFile{lecture15.tex}[Лекция 15]

\newpage
\section{Классификационная задача для линейных отображений}

Абстрактные объекты вроде векторных пространств и линейных отображений между ними становятся более знакомыми после выбора базиса. Пространства превращаются в столбцы, а отображения в матрицы. Но так как базис выбирать можно по-разному, то и матрицы в такой ситуации получаются черт знает какими. Основной вопрос классификационной задачи: как понять по матрицам, что они задают одно и то же линейное отображение, но в разных базисах. Есть два принципиальных по сложности случая: когда линейное отображение бьет между разными пространствами и когда оно действует в одном пространстве (случай линейного оператора). Разберем их по-отдельности.

\subsection{Классификация для линейных отображения между разными пространствами}

Напомню, что если $\varphi \colon V\to U$ -- линейное отображение между векторными пространствами над некоторым полем $F$. То после выбора базиса $e$ в $V$ и базиса $f$ в $U$ линейное отображение $\varphi$ превращается в матрицу $A\in \operatorname{M}_{m\,n}(F)$, где $n = \dim_F V$ и $m = \dim_F U$. Если же мы выберем другие базисы $e'$ и $f'$ в пространствах $V$ и $U$, соответственно, то $\varphi$ превратится в матрицу $A'$. Если $e' = eC$ и $f' = fD$, где $C\in \operatorname{M}_n(F)$ и $D\in\operatorname{M}_m(F)$ -- матрицы перехода к новым базисам, то $A' = D^{-1}A C$.

\begin{claim}\label{claim::HomClassification}
Пусть $V$ и $U$ -- векторные пространства над полем $F$ размерностей $n$ и $m$, соответственно, и пусть нам даны матрицы $A, B\in \operatorname{M}_{m\,n}(F)$. Тогда следующие условия эквивалентны:
\begin{enumerate}
\item Существует линейное отображение $\varphi\colon V\to U$ и базисы $e$ и $e'$ в $V$, $f$ и $f'$ в $U$ такие, что $A$ будет матрицей $\varphi$ в базисах $e$ и $f$, а $B$ будет матрицей $\varphi$ в базисах $e'$ и $f'$.

\item $\rk A = \rk B$.
\end{enumerate}
\end{claim}
\begin{proof}
(1)$\Rightarrow$(2). 
Здесь есть два доказательства: идейное и техническое. Я приведу оба. Давайте начнем с технического. Оно проще в понимании.

\textbf{Техническое доказательство.} Если такой $\varphi$ и базисы существуют, то $B = D^{-1}AC$ для некоторых невырожденных матриц $C$ и $D$ подходящего размера. Тогда мы знаем по утверждению~\ref{claim::rkInvariance}, что $\rk A = \rk B$, так как ранг не меняется при умножении на обратимую матрицу слева и справа.

\textbf{Идейное доказательство.} Если зафиксировать базисы $e$ и $f$ в пространствах $V$ и $U$ соответственно, то $\varphi \colon V\to U$ превращается в $A\colon F^n \to F^m$. Тогда образ $\varphi$ совпадает с линейной оболочкой столбцов матрицы $A$. А значит $\rk A = \dim_F \Im \varphi$. Аналогично, $\rk B = \dim_F \Im \varphi$.

(2)$\Rightarrow$(1). Нам дано, что у матриц $A$ и $B$ равны ранги, а нам надо построить линейное отображение $\varphi\colon V\to U$ и еще пары базисов, чтобы в них матрицы $\varphi$ совпали с $A$ и $B$.

Так как $\rk A = \rk B$, мы можем найти обратимую матрицу $D\in \operatorname{M}_m(F)$ и обратимую матрицу $C\in \operatorname{M}_n(F)$ такие, что $B = D^{-1}AC$.  Действительно, мы можем преобразованиями строк и столбцов матрицу $A$ привести к виду $R = \left(\begin{smallmatrix}{E}&{0}\\{0}&{0}\end{smallmatrix}\right)$, где $E$ имеет размер $\rk A$. То есть $A = D_1 R C_1$. Аналогично, $B = D_2 R C_2$. Выразим $R$ из первого равенства и подставим во второе. Получим требуемое.

Теперь выберем произвольный базис $e$ в $V$ и произвольный базис $f$ в $U$. Чтобы задать линейное отображение из $V$ в $U$ нам надо отправить каждый базисный вектор из $e$ куда-то в $U$ (утверждение~\ref{claim::LinMapExist}). Сделаем это так: $\varphi e = f A$. Тогда мы задали линейное отображение $\varphi \colon V\to U$ такое, что в базисах $e$ и $f$ он имеет матрицу $A$. 

Далее положим $e' = eC$ и $f' = fD$. По утверждению~\ref{claim::BasisClassification} о классификации базисов, из обратимости $C$ и $D$ следует, что $e'$ и $f'$ -- тоже базисы. Тогда оператор $\varphi$ в этих базисах будет иметь матрицу $D^{-1}AC$, которая равна $B$ по построению. Мы сделали все, что требовалось.
\end{proof}


\subsection{Анонс классификационной задачи для линейных операторов}

Пусть теперь $\varphi\colon V\to V$ -- линейный оператор, т.е. линейное отображение из векторного пространства в себя. Тогда при выборе базиса $e$ в $V$ наш оператор превращается в матрицу $A\in \operatorname{M}_n(F)$, где $n = \dim_F V$. Если же мы выберем другой базис $e'$ в $V$ такой, что $e' = eC$ для некоторой обратимой $C \in \operatorname{M}_n(F)$. То матрица $\varphi$ в базисе $e'$ будет $C^{-1}AC$. Заметим сложность ситуации. Мы теперь не можем независимо домножать нашу матрицу с разных сторон на разные матрицы. Если думать в терминах элементарных преобразований, мы теперь должны неким сложным образом согласовывать преобразования строк и столбцов. Из-за этих ограничений кустарными методами (вроде подбора элементарных преобразований) для приведения матрицы в хороший вид нам обойтись не получится. Более того, степень <<хорошести>> нашей матрицы будет сильно зависеть от свойств поля $F$ над которым определены наши векторные пространства. А так как элементарные преобразования ничего не знают про свойства поля, то это автоматически означает, что не мы такие неумелые, что не смогли воспользоваться элементарными преобразованиями, а этот метод в лоб просто не работает.

Для преодоления сложившихся трудностей в случае оператора приходится привлекать более продвинутую технику. К такой технике как раз и относятся собственные векторы и значения, собственные и корневые подпространства. Кульминацией для нас будет теорема о жордановой нормальной форме. Формулировать мы ее пока не будем, но обсудим некоторые стратегические соображения.

Для чего вообще меняется базис? Для того, чтобы сделать вид матрицы линейного оператора максимально простым. Тогда заменив его простой матрицей, его будет проще изучать. В идеале простой вид -- это когда много нулей. Самый желанный для нас вид -- диагональный. Было бы еще лучше, если бы можно было сделать матрицу диагональной с единицами и нулями на диагонали, но это совсем не возможно. Например, если оператор не вырожден, то его определитель не меняется, а он совпадает с произведением диагональных элементов матрицы. То есть скалярную матрицу $\lambda E$ никогда нельзя сделать единичной $E$ путем замены базиса (это так же видно из формулы замены) если $\lambda \neq 1$.

 Оказывается и диагональной можно сделать не всякую матрицу путем сопряжения, то есть не всякий линейный оператор приводится к диагональному виду в каком-то базисе. Потому один из первых вопросов, которым мы хотим заняться -- это вопрос: когда линейный оператор задается диагональной матрицей в некотором базисе. 

\subsection{Диагонализуемость линейного оператора}

\begin{definition}
Пусть $\varphi\colon V\to V$ -- линейный оператор над некоторым полем $F$. Будем говорить, что $\varphi$ диагонализуется или диагонализируемый, если в некотором базисе его матрица является диагональной.
\end{definition}

\begin{claim}\label{claim::EigenRootInd}
Пусть $\varphi\colon V\to V$ -- линейный оператор в некотором векторном пространстве над полем $F$ и пусть $\lambda_1,\ldots,\lambda_k\in F$ -- различные числа. Тогда
\begin{enumerate}
\item Пространства $V_{\lambda_1},\ldots,V_{\lambda_k}$ линейно независимы.\footnote{Определение линейное независимости подпространств~\ref{def::IndepSpaces}.}${}^{,\,}$\footnote{Прошу обратить внимание, что линейно независимые подпространства могут быть нулевыми или часть из них может быть нулевыми.}

\item Пространства $V^{\lambda_1},\ldots,V^{\lambda_k}$ линейно независимы.

\end{enumerate}
\end{claim}
\begin{proof}
Я докажу только первое утверждение для собственных подпространств. Случай корневых оставлю в качестве упражнения.

Пусть $u_1\in V_{\lambda_1},\ldots,u_k\in V_{\lambda_k}$ -- произвольные ненулевые векторы такие, что $u_1 + \ldots + u_k = 0$. Применим к этому равенству оператор $\varphi - \lambda_1\Identity$. Тогда $u_1$ занулится, а $(\varphi - \lambda_1\Identity)u_i = (\lambda_i - \lambda_1)u_i$ будет ненулевым вектором из $V_{\lambda_i}$ при $i \neq 1$. Обозначим эти векторы за $u_2',\ldots,u_k'$. Тогда мы доказали, что если у нас дана сумма из $k$ ненулевых векторов $u_1+\ldots+u_k = 0$, то мы можем получить более короткую сумму из $k - 1$ вектора $u_2'+\ldots+u_k' = 0$.

%Теперь утверждение получается так. Пусть собственные подпространства линейно зависимы. Рассмотрим самую короткую сумму ненулевых векторов из них, которая равна нулю, скажем, $u_1 + \ldots + u_k =0$. Тогда по предыдущему мы можем получить еще более короткую сумму из ненулевых векторов, противоречие.
%
%
%
%Тогда применим к этому равенству оператор $\varphi$. Каждый вектор  $u_i$ умножится на коэффициент $\lambda_i$. Получим $\lambda_1 u_1 + \ldots + \lambda_k u_k = 0$. Теперь продолжим применять к нашим векторам оператор $\varphi$, в результате получим набор равенств
%\[
%\begin{aligned}
%u_1 + &\ldots + u_k = 0\\
%\lambda_1 u_1 + &\ldots + \lambda_k u_k = 0\\
%&\ldots \\
%\lambda^{k-1}_1 u_1 + &\ldots+ \lambda^{k-1}_k u_k = 0
%\end{aligned}
%\]
%Тогда все эти равенства можно записать в матричном виде так
%\[
%\begin{pmatrix}
%{u_1}&{u_2}&{\ldots}&{u_k}\\
%\end{pmatrix}
%\begin{pmatrix}
%{1}&{\lambda_1}&{\ldots}&{\lambda_1^{k-1}}\\
%{1}&{\lambda_2}&{\ldots}&{\lambda_2^{k-1}}\\
%{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
%{1}&{\lambda_k}&{\ldots}&{\lambda_k^{k-1}}\\
%\end{pmatrix}
%=0
%\]
%Определитель матрицы справа равен $\prod_{i<j}(\lambda_i - \lambda_j)$. Значит, если все $\lambda_i$ различны, то матрица обратима. Отсюда следует, что все $u_i = 0$. Что и требовалось.
\end{proof}

\begin{claim}\label{claim::DiagCrit}
Пусть $\varphi\colon V\to V$ -- некоторый линейный оператор в векторном пространстве над полем $F$. Тогда следующие утверждения эквивалентны:
\begin{enumerate}
\item Оператор $\varphi$ диагонализуем.
\item Существует базис из собственных векторов.
\item Существует разложение $V = V_{\lambda_1}\oplus \ldots \oplus V_{\lambda_k}$.
\item 
\begin{enumerate}
\item Характеристический многочлен раскладывается на линейные множители 
\[
\chi_{\varphi}(t) = (t - \lambda_1)^{r_1} \ldots (t - \lambda_k)^{r_k}
\]
\item для каждого $i$ верно $\dim_F V_{\lambda_i} = r_i$.
\end{enumerate}
\end{enumerate}
\end{claim}
\begin{proof}
(1)$\Leftrightarrow$(2). Пусть $e$ -- некоторый базис $V$. Тогда $\varphi e = e A$, где $A$ -- матрица $\varphi$ в базисе $e$. По определению $\varphi$ диагонализуем в базисе $e$ тогда и только тогда, когда $A$ диагональная. С другой стороны, все векторы в $e$ собственные тогда и только тогда, когда $A$ диагональная. 

(2)$\Rightarrow$(3). Пусть $e$ -- базис из собственных векторов и $\varphi e = e A$, где $A$ -- диагональная с числами $\lambda_1,\ldots,\lambda_k$ на диагонали (эти числа могут повторяться). Для удобства переупорядочим вектора так, чтобы одинаковые числа $\lambda_i$ шли по-порядку. Тогда базис $e$ можно разеделить на части $e = e_1 \sqcup \ldots \sqcup e_k$, где все векторы из $e_i$ являются собственными с собственным значением $\lambda_i$. Значит $\langle v\mid v\in e_i\rangle\subseteq V_{\lambda_i}$. А значит 
\[
V = \langle e \rangle = \sum_i \langle e_i\rangle \subseteq  \sum_i V_{\lambda_i} \subseteq V
\]
То есть мы показали, что $V = \sum_i V_{\lambda_i}$ является суммой. С другой стороны, утверждение~\ref{claim::EigenRootInd} гарантирует, что векторные подпространства $V_{\lambda_i}$ линейно независимы, а значит сумма прямая (одно из эквивалентных определений утверждению~\ref{claim::DirectSum}).

(3)$\Rightarrow$(2). Пусть $V = V_{\lambda_1}\oplus \ldots \oplus V_{\lambda_k}$ и пусть $e_i$ -- какой-нибудь базис $V_{\lambda_i}$. Тогда по одному из эквивалентных определений прямой суммы (утверждение~\ref{claim::DirectSum}) $e = e_1 \sqcup\ldots \sqcup e_k$ будет базисом для $V$. Тогда это и есть базис из собственных векторов.

(3)$\Rightarrow$(4). Выберем как в предыдущем пункте базис $e_i$ в каждом слагаемом $V_{\lambda_i}$. Тогда $|e_i| = \dim_F V_{\lambda_i}$. Запишем матрицу нашего оператора в этом базисе в блочном виде
\[
\varphi(e_1,\ldots,e_k)
=
(e_1,\ldots,e_k)
\begin{pmatrix}
{\lambda_1 E}&{}&{}\\
{}&{\ddots}&{}\\
{}&{}&{\lambda_k E}
\end{pmatrix}
\]
где размеры блоков равны в точности $r_i=|e_i| = \dim_F V_{\lambda_i}$. Тогда характеристический многочлен $\chi_\varphi(t) = (t-\lambda_1)^{r_1}\ldots(t-\lambda_k)^{r_k}$. Как мы видим многочлен разложился на линейные множители и кратности корней совпали с размерностями $V_{\lambda_i}$.

(4)$\Rightarrow$(3). Пусть $\lambda_1,\ldots,\lambda_k$ -- все корни характеристического многочлена. Тогда по утверждению~\ref{claim::EigenRootInd} сумма $V_{\lambda_1}+\ldots +V_{\lambda_k}$ всегда прямая. То есть мы имеем $V_{\lambda_1}\oplus \ldots \oplus V_{\lambda_k}\subseteq V$. И осталось лишь проверить равенство. Для этого посчитаем размерности. С одной стороны $\dim_F V = \deg \chi_\varphi$ по определению. С другой стороны размерность левой части есть
\[
\sum_{i}\dim_F V_{\lambda_i} = \sum_i r_i = \deg_\varphi
\]
В первом равенстве мы воспользовались вторым условием, а во втором равенстве первым (если многочлен разложился на линейные множители, то сумма кратностей его корней равна степени). Значит обе размерности совпали и пространства оказались равны.
\end{proof}

\paragraph{Примеры}
\begin{enumerate}
\item Рассмотрим оператор $A\colon \mathbb R^2 \to \mathbb R^2$ по правилу $x\mapsto Ax$, где $A =\left( \begin{smallmatrix}{a}&{-b}\\{b}&{a}\end{smallmatrix}\right)$, $a, b\in\mathbb R$. Если $b\neq 0$ то этот оператор не диагонализуется, потому что его хар многочлен имеет только комплексные корни $a+bi$ и $a - bi$ и не имеет вещественных. Значит не выполняется пункт 4(a).

\item Теперь рассмотрим оператор заданный той же матрицей, но в случае комплексного векторного пространства $A\colon \mathbb C^2 \to \mathbb C^2$ по правилу $x\mapsto Ax$, где $A =\left( \begin{smallmatrix}{a}&{-b}\\{b}&{a}\end{smallmatrix}\right)$, $a, b\in\mathbb R$. Этот оператор диагонализуется и в некотором базисе записывается в виде $\left( \begin{smallmatrix}{a + bi}&{0}\\{0}&{a-bi}\end{smallmatrix}\right)$.

\item Теперь рассмотрим оператор $A\colon \mathbb C^2 \to \mathbb C^2$ по правилу $x\mapsto Ax$, где $A =\left( \begin{smallmatrix}{0}&{1}\\{0}&{0}\end{smallmatrix}\right)$. Тогда его характеристический многочлен $\chi_A(t) = t^2$ раскладывается на линейные множители. Число $\lambda = 0$ является единственной точкой спектра, то есть это единственное собственное значение. Собственное подпространство $V_\lambda$ для $\lambda = 0$ задается $\{x\in \mathbb C^2\mid Ax = 0\}$, которое совпадает с $\langle e_1 \rangle$. То есть $\dim V_\lambda$ не равно кратности корня, то есть он не диагонализуется даже над $\mathbb C$.
\end{enumerate}

Таким образом мы видим, что диагонализуемость зависит от поля. Часть причин недиагонализуемости -- плохой выбор поля. В этом случае в утверждении~\ref{claim::DiagCrit} не выполняется условие 4(a). Такая проблема решается расширением поля до алгебраически замкнутого поля (так всегда можно сделать). Но последний пример показывает, что существуют операторы, которые не диагонализуются над любым полем. Это понастоящему недиагонализуемые операторы. А действительно важное препятствие к диагонализуемости -- это условие 4(b). Другими словами условие 4(b) означает, что размерность собственного подпространства должна совпасть с кратностью соответствующего собственного значения.

